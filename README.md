# NZ Flyfishing Web Scraper

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://img.shields.io/badge/tests-131%20passing-success)](tests/)
[![Coverage](https://img.shields.io/badge/coverage-51%25-orange)](htmlcov/index.html)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**A polite, ethical web scraper for New Zealand fishing information** - designed for personal research and educational purposes.

---

## Overview

The **NZ Flyfishing Web Scraper** systematically collects structured data about fishing regions, rivers, recommended flies, and regulations from New Zealand fishing websites. Built with compliance and ethics as core principles, it respects robots.txt, enforces rate limiting, and maintains complete audit trails.

### Key Features

- **ğŸ—ºï¸ Regional Discovery**: Automatically discovers all fishing regions
- **ğŸï¸ River Cataloging**: Extracts river names, descriptions, and metadata
- **ğŸ£ Fly Recommendations**: Parses fly patterns, sizes, and colors
- **ğŸ“œ Regulation Tracking**: Captures bag limits, seasons, and restrictions
- **âš¡ Smart Caching**: Avoids redundant requests with TTL-based HTML caching
- **ğŸ¤ Polite Crawling**: 3-second delays, exponential backoff, halt on errors
- **ğŸ”’ Compliance First**: Respects robots.txt, comprehensive logging, no inference
- **ğŸ“Š SQLite Storage**: Structured database with raw data preservation

---

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/fly_fishing_log.git
cd fly_fishing_log

# Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r config/requirements.txt

# Verify installation
pytest tests/ -v
```

### Basic Usage

```bash
# 1. Discover all regions
python -m src.cli discover --regions

# 2. Discover rivers in all regions
python -m src.cli discover --rivers --all

# 3. Extract detailed river information
python -m src.cli scrape-details --all

# 4. Query the database
sqlite3 data/nzfishing.db "SELECT name, slug FROM regions LIMIT 5;"
```

ğŸ“˜ **Full Guide**: See [docs/quickstart.md](docs/quickstart.md)

---

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CLI (cli.py)                       â”‚
â”‚         Command Interface & Workflow Orchestration      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fetcher (fetcher.py)â”‚         â”‚  Parser (parser.py)  â”‚
â”‚  - HTTP Requests      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  - HTML Parsing      â”‚
â”‚  - Rate Limiting      â”‚         â”‚  - Field Extraction  â”‚
â”‚  - Caching            â”‚         â”‚  - Data Validation   â”‚
â”‚  - robots.txt         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
            â”‚                                â”‚
            â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Logger (logger.py)   â”‚         â”‚ Storage (storage.py) â”‚
â”‚  - JSON Logging       â”‚         â”‚  - SQLite Operations â”‚
â”‚  - Audit Trail        â”‚         â”‚  - Schema Management â”‚
â”‚  - Compliance Trackingâ”‚         â”‚  - Data Integrity    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ“ **Details**: See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)

---

## Database Schema

### Entity Relationships

```
regions (1) â”€â”€â”€â”€ (N) rivers (1) â”€â”€â”€â”€ (N) flies
                         â”‚
                         â”‚
                         â””â”€â”€â”€â”€ (N) regulations
                         â”‚
                         â””â”€â”€â”€â”€ (N) sections
```

### Core Tables

| Table         | Records | Purpose                          |
|---------------|---------|----------------------------------|
| `regions`     | ~20     | Fishing regions (e.g., Northland)|
| `rivers`      | ~200+   | Rivers and streams per region    |
| `flies`       | ~500+   | Recommended fly patterns         |
| `regulations` | ~300+   | Bag limits, seasons, restrictions|
| `sections`    | ~100+   | River sections (Upper/Lower)     |

ğŸ“Š **Schema**: See [docs/DATABASE.md](docs/DATABASE.md)

---

## Compliance & Ethics

### Core Principles (Article 1-11)

1. **Personal Use Only**: Educational and research purposes
2. **Polite Crawling**: 3-second delays, respect rate limits
3. **robots.txt Compliance**: Auto-loads and respects directives
4. **No Inference**: Extract only explicit content
5. **Raw Data Preservation**: Original HTML always stored
6. **Complete Logging**: All requests timestamped and auditable
7. **Halt on Errors**: Stop on 3+ consecutive 5xx errors
8. **Privacy**: No personal data collection
9. **Attribution**: Source URLs tracked for all data

### Rate Limiting

- **Request Delay**: 3.0 seconds (configurable)
- **Exponential Backoff**: 1s, 2s, 4s on errors
- **Cache Bypass**: Cached responses = instant (no delay)
- **Halt Condition**: 3+ consecutive 5xx â†’ stop execution

ğŸ“œ **Full Guidelines**: See [docs/COMPLIANCE.md](docs/COMPLIANCE.md)

---

## Testing

### Test Coverage

| Module          | Coverage | Tests |
|-----------------|----------|-------|
| `fetcher.py`    | 91.60%   | 22    |
| `parser.py`     | 83.33%   | 28    |
| `logger.py`     | 83.33%   | 13    |
| `storage.py`    | 80.00%   | 35    |
| `exceptions.py` | 100.00%  | 8     |
| **Total**       | **51.49%** | **131** |

### Run Tests

```bash
# All tests with coverage
pytest --cov=src --cov-report=term-missing --cov-report=html

# Specific test suites
pytest tests/unit/ -v           # Unit tests
pytest tests/integration/ -v    # Integration tests
pytest tests/contract/ -v       # Contract tests

# View HTML coverage report
open htmlcov/index.html
```

---

## Project Structure

```
fly_fishing_log/
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ cli.py                 # Command-line interface
â”‚   â”œâ”€â”€ fetcher.py             # HTTP client + caching
â”‚   â”œâ”€â”€ parser.py              # HTML parsing logic
â”‚   â”œâ”€â”€ storage.py             # SQLite operations
â”‚   â”œâ”€â”€ logger.py              # JSON logging
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ models.py              # Data models
â”‚   â””â”€â”€ exceptions.py          # Custom exceptions
â”œâ”€â”€ tests/                      # Test suites
â”‚   â”œâ”€â”€ unit/                  # Unit tests
â”‚   â”œâ”€â”€ integration/           # Integration tests
â”‚   â””â”€â”€ contract/              # Contract tests
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ nzfishing_config.yaml  # Main config
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ quickstart.md          # Quick start guide
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System design
â”‚   â”œâ”€â”€ DATABASE.md            # Schema reference
â”‚   â”œâ”€â”€ COMPLIANCE.md          # Ethical guidelines
â”‚   â””â”€â”€ TROUBLESHOOTING.md     # Common issues
â”œâ”€â”€ data/                       # SQLite database
â”‚   â””â”€â”€ nzfishing.db           # Scraped data
â”œâ”€â”€ logs/                       # JSON logs
â”‚   â””â”€â”€ nzfishing.log          # Audit trail
â”œâ”€â”€ cache/                      # HTML cache
â”‚   â””â”€â”€ *.html                 # Cached responses
â”œâ”€â”€ specs/                      # Spec-Kit methodology
â”‚   â””â”€â”€ 001-nzfishing-scraper/ # Project specifications
â”‚       â”œâ”€â”€ spec.md            # Requirements
â”‚       â”œâ”€â”€ plan.md            # Technical plan
â”‚       â”œâ”€â”€ tasks.md           # Implementation tasks
â”‚       â”œâ”€â”€ data-model.md      # Entity model
â”‚       â””â”€â”€ contracts/         # API contracts
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml             # pytest configuration
â””â”€â”€ README.md                  # This file
```

---

## Configuration

### Config File: `config/nzfishing_config.yaml`

```yaml
base_url: "https://fishandgame.org.nz"
user_agent: "nzfishing-scraper/1.0 (educational/personal use)"

# Rate limiting
request_delay: 3.0
jitter_max: 0.5

# Caching
cache_dir: "cache/"
cache_ttl: 86400  # 24 hours

# Retry logic
max_retries: 3
retry_backoff: [1, 2, 4]
halt_on_consecutive_5xx: 3

# Paths
database_path: "data/nzfishing.db"
log_path: "logs/nzfishing.log"
```

---

## Troubleshooting

### Common Issues

| Issue                  | Cause                     | Solution                          |
|------------------------|---------------------------|-----------------------------------|
| Connection Refused     | Site down / blocking      | Wait 5-10 min, check base_url     |
| HaltError (3x 5xx)     | Server errors             | Wait 30-60 min before retry       |
| No data extracted      | HTML structure changed    | Update CSS selectors in config    |
| Slow performance       | Rate limiting enforced    | Expected; use caching             |
| Database locked        | Multiple processes        | Close other scraper instances     |

ğŸ› ï¸ **Full Guide**: See [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)

---

## Success Criteria (Phase 9 Validation)

- âœ… **SC-001**: Regional discovery extracts 20+ regions
- âœ… **SC-002**: River discovery extracts 50+ rivers per region
- âœ… **SC-003**: Detail extraction captures flies, regulations, descriptions
- âœ… **SC-004**: 3-second delays enforced between requests
- âœ… **SC-005**: Complete JSON logging with timestamps
- âœ… **SC-006**: robots.txt compliance validated
- âœ… **SC-007**: Halt on 3+ consecutive 5xx errors
- âœ… **SC-008**: Caching reduces duplicate requests
- âœ… **SC-009**: Raw data preserved immutably
- âœ… **SC-010**: Offline queries work without internet

---

## Development Methodology

This project was built using **Spec-Kit**, a systematic approach to software development:

1. **Specification** ([spec.md](specs/001-nzfishing-scraper/spec.md)): Requirements and user stories
2. **Planning** ([plan.md](specs/001-nzfishing-scraper/plan.md)): Tech stack and architecture
3. **Task Breakdown** ([tasks.md](specs/001-nzfishing-scraper/tasks.md)): 72 tasks across 9 phases
4. **Contracts** ([contracts/](specs/001-nzfishing-scraper/contracts/)): API and test specifications
5. **Implementation**: Phase-by-phase execution with TDD

**Current Status**: Phase 9 (Polish & Cross-Cutting) - 61/72 tasks complete (85%)

---

## Roadmap

### Completed Phases âœ…

- Phase 1: Project Setup (7/7 tasks)
- Phase 2: Foundational Modules (18/18 tasks)
- Phase 3: US1 Region Discovery (8/8 tasks)
- Phase 5: US3 River Details (8/9 tasks)
- Phase 6: US4 Polite Crawling (8/8 tasks)
- Phase 7: US5 HTML Caching (6/6 tasks)
- Phase 9: Polish & Integration (3/10 tasks) - **IN PROGRESS**

### Pending Features ğŸš§

- Phase 4: US2 River Discovery (fixes needed, 7/32 tests failing)
- Phase 8: US6 PDF Export (deferred)

### Future Enhancements ğŸ”®

- Web UI for data visualization
- Export to CSV/JSON formats
- GIS integration (map overlays)
- Multi-site support (configurable parsers)
- Incremental updates (delta scraping)

---

## Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/your-feature`
3. **Write tests**: Maintain â‰¥80% coverage
4. **Follow code style**: `black src/ tests/`
5. **Run tests**: `pytest tests/ -v`
6. **Submit pull request** with clear description

### Code Style

- **Formatter**: Black (line length 100)
- **Linter**: Flake8
- **Type Hints**: Required for public APIs
- **Docstrings**: Google style

---

## License

This project is licensed under the **MIT License**.

**Disclaimer**: This scraper is provided for educational and personal research purposes only. Users are solely responsible for compliance with applicable laws, terms of service, and ethical web scraping practices. The authors assume no liability for misuse.

---

## Acknowledgments

- **Spec-Kit Methodology**: Systematic development approach
- **pytest**: Testing framework
- **BeautifulSoup4**: HTML parsing
- **SQLite**: Embedded database
- **httpbin.org**: Testing infrastructure

---

## Support

- **Documentation**: See [docs/](docs/) directory
- **Issues**: [GitHub Issues](https://github.com/yourusername/fly_fishing_log/issues)
- **Tests**: Run `pytest -v` to validate installation

---

**Built with â¤ï¸ for the fishing community. Scrape responsibly. ğŸ£**
